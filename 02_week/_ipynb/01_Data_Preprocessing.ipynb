{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Import libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import time\n", "import pickle\n", "import pandas as pd\n", "import pyarrow.parquet as pa\n", "from sklearn.feature_extraction import DictVectorizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Start time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_time = time.time()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting path to the data directory"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CURRENT_DIRECTORY = os.getcwd()\n", "PARENT_DIRECTORY = os.path.dirname(CURRENT_DIRECTORY)\n", "DATA_PATH = os.path.join(PARENT_DIRECTORY, '_data')\n", "PICKLE_PATH = os.path.join(CURRENT_DIRECTORY, '_pickle')\n", "DEST_PATH = os.path.join(CURRENT_DIRECTORY, 'DEST_PATH')  # Create a specific directory"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ensure the DEST_PATH directory exists"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not os.path.exists(DEST_PATH):\n", "    print(f\"Creating directory: {DEST_PATH}\")\n", "    os.makedirs(DEST_PATH)\n", "else:\n", "    print(f\"Directory already exists: {DEST_PATH}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorise = DictVectorizer()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Notes:<br>\n", "    1. We shall use the code of Data Pre-processing written for Week-01.<br>\n", "    2. Here we are using Yellow taxi data of January, February, and March months.<br>\n", "    3. train => January, validation => February, test => March.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def path_join(train, val, test):\n", "    \"\"\"\n", "    Join the paths for the train, validation, and test datasets.\n", "    Args:\n", "    train (str): Filename for the train dataset.\n", "    val (str): Filename for the validation dataset.\n", "    test (str): Filename for the test dataset.\n", "    Returns:\n", "    list: List containing the full paths for the train, validation, and test datasets.\n", "    \"\"\"\n", "    train_data_path = os.path.join(DATA_PATH, train)\n", "    val_data_path = os.path.join(DATA_PATH, val)\n", "    test_data_path = os.path.join(DATA_PATH, test)\n", "    return [train_data_path, val_data_path, test_data_path]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_data(data):\n", "    \"\"\"\n", "    Read the data from a file and return it as a pandas DataFrame.\n", "    Args:\n", "    data (str): Path to the data file.\n", "    Returns:\n", "    pd.DataFrame: DataFrame containing the data.\n", "    \"\"\"\n", "    if data.endswith('.parquet'):\n", "        data = pa.read_table(data)\n", "        df = data.to_pandas()  # Converting to pandas DataFrame\n", "        df.columns = df.columns.str.lower()\n", "        return df\n", "    elif data.endswith('.csv'):\n", "        df = pd.read_csv(data)\n", "        df.columns = df.columns.str.lower()\n", "        return df\n", "    else:\n", "        return 'Not valid format'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_pickle(obj, filename: str):\n", "    \"\"\"\n", "    Save an object to a pickle file.\n", "    Args:\n", "    obj: Object to be saved.\n", "    filename (str): Name of the file where the object will be saved.\n", "    \"\"\"\n", "    with open(filename, \"wb\") as f_out:\n", "        return pickle.dump(obj, f_out)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calculate_duration(data):\n", "    \"\"\"\n", "    Calculate the duration of each trip in minutes.\n", "    Args:\n", "    data (pd.DataFrame): DataFrame containing the trip data.\n", "    Returns:\n", "    pd.DataFrame: DataFrame with an added 'duration' column.\n", "    \"\"\"\n", "    data['duration'] = pd.to_datetime(data['lpep_dropoff_datetime']) - pd.to_datetime(data['lpep_pickup_datetime'])\n", "    data['duration'] = data['duration'].dt.total_seconds() / 60  # Convert seconds to minutes\n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def outliers(data):\n", "    \"\"\"\n", "    Filter out trips with durations outside the range [1, 60] minutes.\n", "    Args:\n", "    data (pd.DataFrame): DataFrame containing the trip data.\n", "    Returns:\n", "    pd.DataFrame: DataFrame with outliers removed.\n", "    \"\"\"\n", "    data_outliers = data[(data['duration'] >= 1) & (data['duration'] <= 60)]\n", "    data_outliers['pulocationid'] = data_outliers['pulocationid'].astype(str)\n", "    data_outliers['dolocationid'] = data_outliers['dolocationid'].astype(str)\n", "    return data_outliers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convert_to_dict(data_outliers):\n", "    \"\"\"\n", "    Convert the DataFrame to a list of dictionaries for vectorization.\n", "    Args:\n", "    data_outliers (pd.DataFrame): DataFrame containing the filtered data.\n", "    Returns:\n", "    list: List of dictionaries representing the data.\n", "    \"\"\"\n", "    return data_outliers[['pulocationid', 'dolocationid', 'trip_distance']].to_dict(orient='records')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fit_transform_(df_dict):\n", "    \"\"\"\n", "    Fit and transform the data using DictVectorizer.\n", "    Args:\n", "    df_dict (list): List of dictionaries representing the data.\n", "    Returns:\n", "    scipy.sparse.csr_matrix: Transformed data.\n", "    \"\"\"\n", "    return vectorise.fit_transform(df_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fit_(df_dict):\n", "    \"\"\"\n", "    Transform the data using an already fitted DictVectorizer.\n", "    Args:\n", "    df_dict (list): List of dictionaries representing the data.\n", "    Returns:\n", "    scipy.sparse.csr_matrix: Transformed data.\n", "    \"\"\"\n", "    return vectorise.transform(df_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pre_processing(data, choice):\n", "    \"\"\"\n", "    Pre-process the data by calculating duration, removing outliers, and vectorizing the data.\n", "    Args:\n", "    data (pd.DataFrame): DataFrame containing the trip data.\n", "    choice (int): Choice for vectorization (0 for training data, 1 for validation/test data).\n", "    Returns:\n", "    tuple: Tuple containing the vectorized data and the DataFrame with outliers removed.\n", "    \"\"\"\n", "    data = calculate_duration(data)\n", "    data_outliers = outliers(data)\n", "    df_dict = convert_to_dict(data_outliers)\n", "    if choice == 0:\n", "        X_train = fit_transform_(df_dict)\n", "        return X_train, data_outliers\n", "    elif choice == 1:\n", "        X_val = fit_(df_dict)\n", "        return X_val, data_outliers\n", "    else:\n", "        return 'Enter Choice 0 or 1'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(train, val, test):\n", "    \"\"\"\n", "    Main function to execute the data pre-processing pipeline.\n", "    Args:\n", "    train (str): Filename for the train dataset.\n", "    val (str): Filename for the validation dataset.\n", "    test (str): Filename for the test dataset.\n", "    \"\"\"\n", "    data_path_files = path_join(train, val, test)\n", "    df_train = read_data(data_path_files[0])  # Read January data\n", "    df_val = read_data(data_path_files[1])  # Read February data\n", "    df_test = read_data(data_path_files[2])  # Read March data\n", "    X_train, df_train = pre_processing(df_train, choice=0)\n", "    X_val, df_val = pre_processing(df_val, choice=1)\n", "    X_test, df_test = pre_processing(df_test, choice=1)\n", "    y_train = df_train['duration']\n", "    y_val = df_val['duration']\n", "    y_test = df_test['duration']\n\n", "    # Save DictVectorizer and datasets\n", "    save_pickle(vectorise, os.path.join(DEST_PATH, \"vectorise.pkl\"))\n", "    save_pickle((X_train, y_train), os.path.join(DEST_PATH, \"train.pkl\"))\n", "    save_pickle((X_val, y_val), os.path.join(DEST_PATH, \"val.pkl\"))\n", "    save_pickle((X_test, y_test), os.path.join(DEST_PATH, \"test.pkl\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    # File Names\n", "    january_file_name = 'green_tripdata_2023-01.parquet'\n", "    february_file_name = 'green_tripdata_2023-02.parquet'\n", "    march_file_name = 'green_tripdata_2023-03.parquet'\n", "    main(january_file_name, february_file_name, march_file_name)\n\n", "    # End time\n", "    end_time = time.time()\n", "    print(f\"Total time taken to run the script: {end_time - start_time} seconds\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}